{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3459f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1319210416.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    [docs]def utf8_decoder(s):\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "import warnings\n",
    "\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.serializers import AutoBatchedSerializer, PickleSerializer, PairDeserializer, \\\n",
    "    NoOpSerializer\n",
    "from pyspark.streaming import DStream\n",
    "from pyspark.streaming.dstream import TransformedDStream\n",
    "from pyspark.streaming.util import TransformFunction\n",
    "\n",
    "__all__ = ['Broker', 'KafkaMessageAndMetadata', 'KafkaUtils', 'OffsetRange',\n",
    "           'TopicAndPartition', 'utf8_decoder']\n",
    "\n",
    "\n",
    "[docs]def utf8_decoder(s):\n",
    "    \"\"\" Decode the unicode as UTF-8 \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    return s.decode('utf-8')\n",
    "\n",
    "\n",
    "\n",
    "[docs]class KafkaUtils(object):\n",
    "\n",
    "[docs]    @staticmethod\n",
    "    def createStream(ssc, zkQuorum, groupId, topics, kafkaParams=None,\n",
    "                     storageLevel=StorageLevel.MEMORY_AND_DISK_2,\n",
    "                     keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n",
    "        \"\"\"\n",
    "        Create an input stream that pulls messages from a Kafka Broker.\n",
    "\n",
    "        :param ssc:  StreamingContext object\n",
    "        :param zkQuorum:  Zookeeper quorum (hostname:port,hostname:port,..).\n",
    "        :param groupId:  The group id for this consumer.\n",
    "        :param topics:  Dict of (topic_name -> numPartitions) to consume.\n",
    "                        Each partition is consumed in its own thread.\n",
    "        :param kafkaParams: Additional params for Kafka\n",
    "        :param storageLevel:  RDD storage level.\n",
    "        :param keyDecoder:  A function used to decode key (default is utf8_decoder)\n",
    "        :param valueDecoder:  A function used to decode value (default is utf8_decoder)\n",
    "        :return: A DStream object\n",
    "\n",
    "        .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "            See SPARK-21893.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        if kafkaParams is None:\n",
    "            kafkaParams = dict()\n",
    "        kafkaParams.update({\n",
    "            \"zookeeper.connect\": zkQuorum,\n",
    "            \"group.id\": groupId,\n",
    "            \"zookeeper.connection.timeout.ms\": \"10000\",\n",
    "        })\n",
    "        if not isinstance(topics, dict):\n",
    "            raise TypeError(\"topics should be dict\")\n",
    "        jlevel = ssc._sc._getJavaStorageLevel(storageLevel)\n",
    "        helper = KafkaUtils._get_helper(ssc._sc)\n",
    "        jstream = helper.createStream(ssc._jssc, kafkaParams, topics, jlevel)\n",
    "        ser = PairDeserializer(NoOpSerializer(), NoOpSerializer())\n",
    "        stream = DStream(jstream, ssc, ser)\n",
    "        return stream.map(lambda k_v: (keyDecoder(k_v[0]), valueDecoder(k_v[1])))\n",
    "\n",
    "\n",
    "[docs]    @staticmethod\n",
    "    def createDirectStream(ssc, topics, kafkaParams, fromOffsets=None,\n",
    "                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder,\n",
    "                           messageHandler=None):\n",
    "        \"\"\"\n",
    "        Create an input stream that directly pulls messages from a Kafka Broker and specific offset.\n",
    "\n",
    "        This is not a receiver based Kafka input stream, it directly pulls the message from Kafka\n",
    "        in each batch duration and processed without storing.\n",
    "\n",
    "        This does not use Zookeeper to store offsets. The consumed offsets are tracked\n",
    "        by the stream itself. For interoperability with Kafka monitoring tools that depend on\n",
    "        Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.\n",
    "        You can access the offsets used in each batch from the generated RDDs (see\n",
    "\n",
    "        To recover from driver failures, you have to enable checkpointing in the StreamingContext.\n",
    "        The information on consumed offset can be recovered from the checkpoint.\n",
    "        See the programming guide for details (constraints, etc.).\n",
    "\n",
    "        :param ssc:  StreamingContext object.\n",
    "        :param topics:  list of topic_name to consume.\n",
    "        :param kafkaParams: Additional params for Kafka.\n",
    "        :param fromOffsets: Per-topic/partition Kafka offsets defining the (inclusive) starting\n",
    "                            point of the stream (a dictionary mapping `TopicAndPartition` to\n",
    "                            integers).\n",
    "        :param keyDecoder:  A function used to decode key (default is utf8_decoder).\n",
    "        :param valueDecoder:  A function used to decode value (default is utf8_decoder).\n",
    "        :param messageHandler: A function used to convert KafkaMessageAndMetadata. You can assess\n",
    "                               meta using messageHandler (default is None).\n",
    "        :return: A DStream object\n",
    "\n",
    "        .. note:: Experimental\n",
    "        .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "            See SPARK-21893.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        if fromOffsets is None:\n",
    "            fromOffsets = dict()\n",
    "        if not isinstance(topics, list):\n",
    "            raise TypeError(\"topics should be list\")\n",
    "        if not isinstance(kafkaParams, dict):\n",
    "            raise TypeError(\"kafkaParams should be dict\")\n",
    "\n",
    "        def funcWithoutMessageHandler(k_v):\n",
    "            return (keyDecoder(k_v[0]), valueDecoder(k_v[1]))\n",
    "\n",
    "        def funcWithMessageHandler(m):\n",
    "            m._set_key_decoder(keyDecoder)\n",
    "            m._set_value_decoder(valueDecoder)\n",
    "            return messageHandler(m)\n",
    "\n",
    "        helper = KafkaUtils._get_helper(ssc._sc)\n",
    "\n",
    "        jfromOffsets = dict([(k._jTopicAndPartition(helper),\n",
    "                              v) for (k, v) in fromOffsets.items()])\n",
    "        if messageHandler is None:\n",
    "            ser = PairDeserializer(NoOpSerializer(), NoOpSerializer())\n",
    "            func = funcWithoutMessageHandler\n",
    "            jstream = helper.createDirectStreamWithoutMessageHandler(\n",
    "                ssc._jssc, kafkaParams, set(topics), jfromOffsets)\n",
    "        else:\n",
    "            ser = AutoBatchedSerializer(PickleSerializer())\n",
    "            func = funcWithMessageHandler\n",
    "            jstream = helper.createDirectStreamWithMessageHandler(\n",
    "                ssc._jssc, kafkaParams, set(topics), jfromOffsets)\n",
    "\n",
    "        stream = DStream(jstream, ssc, ser).map(func)\n",
    "        return KafkaDStream(stream._jdstream, ssc, stream._jrdd_deserializer)\n",
    "\n",
    "\n",
    "[docs]    @staticmethod\n",
    "    def createRDD(sc, kafkaParams, offsetRanges, leaders=None,\n",
    "                  keyDecoder=utf8_decoder, valueDecoder=utf8_decoder,\n",
    "                  messageHandler=None):\n",
    "        \"\"\"\n",
    "        Create an RDD from Kafka using offset ranges for each topic and partition.\n",
    "\n",
    "        :param sc:  SparkContext object\n",
    "        :param kafkaParams: Additional params for Kafka\n",
    "        :param offsetRanges:  list of offsetRange to specify topic:partition:[start, end) to consume\n",
    "        :param leaders: Kafka brokers for each TopicAndPartition in offsetRanges.  May be an empty\n",
    "            map, in which case leaders will be looked up on the driver.\n",
    "        :param keyDecoder:  A function used to decode key (default is utf8_decoder)\n",
    "        :param valueDecoder:  A function used to decode value (default is utf8_decoder)\n",
    "        :param messageHandler: A function used to convert KafkaMessageAndMetadata. You can assess\n",
    "                               meta using messageHandler (default is None).\n",
    "        :return: An RDD object\n",
    "\n",
    "        .. note:: Experimental\n",
    "        .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "            See SPARK-21893.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        if leaders is None:\n",
    "            leaders = dict()\n",
    "        if not isinstance(kafkaParams, dict):\n",
    "            raise TypeError(\"kafkaParams should be dict\")\n",
    "        if not isinstance(offsetRanges, list):\n",
    "            raise TypeError(\"offsetRanges should be list\")\n",
    "\n",
    "        def funcWithoutMessageHandler(k_v):\n",
    "            return (keyDecoder(k_v[0]), valueDecoder(k_v[1]))\n",
    "\n",
    "        def funcWithMessageHandler(m):\n",
    "            m._set_key_decoder(keyDecoder)\n",
    "            m._set_value_decoder(valueDecoder)\n",
    "            return messageHandler(m)\n",
    "\n",
    "        helper = KafkaUtils._get_helper(sc)\n",
    "\n",
    "        joffsetRanges = [o._jOffsetRange(helper) for o in offsetRanges]\n",
    "        jleaders = dict([(k._jTopicAndPartition(helper),\n",
    "                          v._jBroker(helper)) for (k, v) in leaders.items()])\n",
    "        if messageHandler is None:\n",
    "            jrdd = helper.createRDDWithoutMessageHandler(\n",
    "                sc._jsc, kafkaParams, joffsetRanges, jleaders)\n",
    "            ser = PairDeserializer(NoOpSerializer(), NoOpSerializer())\n",
    "            rdd = RDD(jrdd, sc, ser).map(funcWithoutMessageHandler)\n",
    "        else:\n",
    "            jrdd = helper.createRDDWithMessageHandler(\n",
    "                sc._jsc, kafkaParams, joffsetRanges, jleaders)\n",
    "            rdd = RDD(jrdd, sc).map(funcWithMessageHandler)\n",
    "\n",
    "        return KafkaRDD(rdd._jrdd, sc, rdd._jrdd_deserializer)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_helper(sc):\n",
    "        try:\n",
    "            return sc._jvm.org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper()\n",
    "        except TypeError as e:\n",
    "            if str(e) == \"'JavaPackage' object is not callable\":\n",
    "                KafkaUtils._printErrorMsg(sc)\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _printErrorMsg(sc):\n",
    "        print(\"\"\"\n",
    "________________________________________________________________________________________________\n",
    "\n",
    "  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n",
    "\n",
    "  1. Include the Kafka library and its dependencies with in the\n",
    "     spark-submit command as\n",
    "\n",
    "     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8:%s ...\n",
    "\n",
    "  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n",
    "     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-8-assembly, Version = %s.\n",
    "     Then, include the jar in the spark-submit command as\n",
    "\n",
    "     $ bin/spark-submit --jars <spark-streaming-kafka-0-8-assembly.jar> ...\n",
    "\n",
    "________________________________________________________________________________________________\n",
    "\n",
    "\"\"\" % (sc.version, sc.version))\n",
    "\n",
    "\n",
    "\n",
    "[docs]class OffsetRange(object):\n",
    "    \"\"\"\n",
    "    Represents a range of offsets from a single Kafka TopicAndPartition.\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic, partition, fromOffset, untilOffset):\n",
    "        \"\"\"\n",
    "        Create an OffsetRange to represent range of offsets\n",
    "        :param topic: Kafka topic name.\n",
    "        :param partition: Kafka partition id.\n",
    "        :param fromOffset: Inclusive starting offset.\n",
    "        :param untilOffset: Exclusive ending offset.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        self.topic = topic\n",
    "        self.partition = partition\n",
    "        self.fromOffset = fromOffset\n",
    "        self.untilOffset = untilOffset\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.topic == other.topic\n",
    "                    and self.partition == other.partition\n",
    "                    and self.fromOffset == other.fromOffset\n",
    "                    and self.untilOffset == other.untilOffset)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"OffsetRange(topic: %s, partition: %d, range: [%d -> %d]\" \\\n",
    "               % (self.topic, self.partition, self.fromOffset, self.untilOffset)\n",
    "\n",
    "    def _jOffsetRange(self, helper):\n",
    "        return helper.createOffsetRange(self.topic, self.partition, self.fromOffset,\n",
    "                                        self.untilOffset)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class TopicAndPartition(object):\n",
    "    \"\"\"\n",
    "    Represents a specific topic and partition for Kafka.\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic, partition):\n",
    "        \"\"\"\n",
    "        Create a Python TopicAndPartition to map to the Java related object\n",
    "        :param topic: Kafka topic name.\n",
    "        :param partition: Kafka partition id.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        self._topic = topic\n",
    "        self._partition = partition\n",
    "\n",
    "    def _jTopicAndPartition(self, helper):\n",
    "        return helper.createTopicAndPartition(self._topic, self._partition)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self._topic == other._topic\n",
    "                    and self._partition == other._partition)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return (self._topic, self._partition).__hash__()\n",
    "\n",
    "\n",
    "\n",
    "[docs]class Broker(object):\n",
    "    \"\"\"\n",
    "    Represent the host and port info for a Kafka broker.\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, host, port):\n",
    "        \"\"\"\n",
    "        Create a Python Broker to map to the Java related object.\n",
    "        :param host: Broker's hostname.\n",
    "        :param port: Broker's port.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        self._host = host\n",
    "        self._port = port\n",
    "\n",
    "    def _jBroker(self, helper):\n",
    "        return helper.createBroker(self._host, self._port)\n",
    "\n",
    "\n",
    "\n",
    "class KafkaRDD(RDD):\n",
    "    \"\"\"\n",
    "    A Python wrapper of KafkaRDD, to provide additional information on normal RDD.\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jrdd, ctx, jrdd_deserializer):\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        RDD.__init__(self, jrdd, ctx, jrdd_deserializer)\n",
    "\n",
    "    def offsetRanges(self):\n",
    "        \"\"\"\n",
    "        Get the OffsetRange of specific KafkaRDD.\n",
    "        :return: A list of OffsetRange\n",
    "        \"\"\"\n",
    "        helper = KafkaUtils._get_helper(self.ctx)\n",
    "        joffsetRanges = helper.offsetRangesOfKafkaRDD(self._jrdd.rdd())\n",
    "        ranges = [OffsetRange(o.topic(), o.partition(), o.fromOffset(), o.untilOffset())\n",
    "                  for o in joffsetRanges]\n",
    "        return ranges\n",
    "\n",
    "\n",
    "class KafkaDStream(DStream):\n",
    "    \"\"\"\n",
    "    A Python wrapper of KafkaDStream\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jdstream, ssc, jrdd_deserializer):\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        DStream.__init__(self, jdstream, ssc, jrdd_deserializer)\n",
    "\n",
    "    def foreachRDD(self, func):\n",
    "        \"\"\"\n",
    "        Apply a function to each RDD in this DStream.\n",
    "        \"\"\"\n",
    "        if func.__code__.co_argcount == 1:\n",
    "            old_func = func\n",
    "            func = lambda r, rdd: old_func(rdd)\n",
    "        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer) \\\n",
    "            .rdd_wrapper(lambda jrdd, ctx, ser: KafkaRDD(jrdd, ctx, ser))\n",
    "        api = self._ssc._jvm.PythonDStream\n",
    "        api.callForeachRDD(self._jdstream, jfunc)\n",
    "\n",
    "    def transform(self, func):\n",
    "        \"\"\"\n",
    "        Return a new DStream in which each RDD is generated by applying a function\n",
    "        on each RDD of this DStream.\n",
    "\n",
    "        `func` can have one argument of `rdd`, or have two arguments of\n",
    "        (`time`, `rdd`)\n",
    "        \"\"\"\n",
    "        if func.__code__.co_argcount == 1:\n",
    "            oldfunc = func\n",
    "            func = lambda t, rdd: oldfunc(rdd)\n",
    "        assert func.__code__.co_argcount == 2, \"func should take one or two arguments\"\n",
    "\n",
    "        return KafkaTransformedDStream(self, func)\n",
    "\n",
    "\n",
    "class KafkaTransformedDStream(TransformedDStream):\n",
    "    \"\"\"\n",
    "    Kafka specific wrapper of TransformedDStream to transform on Kafka RDD.\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prev, func):\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        TransformedDStream.__init__(self, prev, func)\n",
    "\n",
    "    @property\n",
    "    def _jdstream(self):\n",
    "        if self._jdstream_val is not None:\n",
    "            return self._jdstream_val\n",
    "\n",
    "        jfunc = TransformFunction(self._sc, self.func, self.prev._jrdd_deserializer) \\\n",
    "            .rdd_wrapper(lambda jrdd, ctx, ser: KafkaRDD(jrdd, ctx, ser))\n",
    "        dstream = self._sc._jvm.PythonTransformedDStream(self.prev._jdstream.dstream(), jfunc)\n",
    "        self._jdstream_val = dstream.asJavaDStream()\n",
    "        return self._jdstream_val\n",
    "\n",
    "\n",
    "[docs]class KafkaMessageAndMetadata(object):\n",
    "    \"\"\"\n",
    "    Kafka message and metadata information. Including topic, partition, offset and message\n",
    "\n",
    "    .. note:: Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0.\n",
    "        See SPARK-21893.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic, partition, offset, key, message):\n",
    "        \"\"\"\n",
    "        Python wrapper of Kafka MessageAndMetadata\n",
    "        :param topic: topic name of this Kafka message\n",
    "        :param partition: partition id of this Kafka message\n",
    "        :param offset: Offset of this Kafka message in the specific partition\n",
    "        :param key: key payload of this Kafka message, can be null if this Kafka message has no key\n",
    "                    specified, the return data is undecoded bytearry.\n",
    "        :param message: actual message payload of this Kafka message, the return data is\n",
    "                        undecoded bytearray.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"Deprecated in 2.3.0. Kafka 0.8 support is deprecated as of Spark 2.3.0. \"\n",
    "            \"See SPARK-21893.\",\n",
    "            DeprecationWarning)\n",
    "        self.topic = topic\n",
    "        self.partition = partition\n",
    "        self.offset = offset\n",
    "        self._rawKey = key\n",
    "        self._rawMessage = message\n",
    "        self._keyDecoder = utf8_decoder\n",
    "        self._valueDecoder = utf8_decoder\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"KafkaMessageAndMetadata(topic: %s, partition: %d, offset: %d, key and message...)\" \\\n",
    "               % (self.topic, self.partition, self.offset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (KafkaMessageAndMetadata,\n",
    "                (self.topic, self.partition, self.offset, self._rawKey, self._rawMessage))\n",
    "\n",
    "    def _set_key_decoder(self, decoder):\n",
    "        self._keyDecoder = decoder\n",
    "\n",
    "    def _set_value_decoder(self, decoder):\n",
    "        self._valueDecoder = decoder\n",
    "\n",
    "    @property\n",
    "    def key(self):\n",
    "        return self._keyDecoder(self._rawKey)\n",
    "\n",
    "    @property\n",
    "    def message(self):\n",
    "        return self._valueDecoder(self._rawMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.streaming import DStream\n",
    "from pyspark.streaming.dstream import TransformedDStream\n",
    "from pyspark.serializers import PickleSerializer, NoOpSerializer\n",
    "from pyspark.streaming.util import TransformFunction\n",
    "\n",
    "__all__ = ['Broker', 'KafkaMessageAndMetadata', 'KafkaUtils', 'OffsetRange', 'TopicAndPartition', 'utf8_decoder']\n",
    "\n",
    "\n",
    "def utf8_decoder(s):\n",
    "    \"\"\" Decode the unicode as UTF-8 \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    return s.decode('utf-8')\n",
    "\n",
    "\n",
    "class KafkaUtils(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def createStream(ssc, zkQuorum, groupId, topics, kafkaParams=None,\n",
    "                     storageLevel=None,\n",
    "                     keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n",
    "        \"\"\"\n",
    "        Create an input stream that pulls messages from a Kafka Broker.\n",
    "\n",
    "        :param ssc:  StreamingContext object\n",
    "        :param zkQuorum:  Zookeeper quorum (hostname:port,hostname:port,..).\n",
    "        :param groupId:  The group id for this consumer.\n",
    "        :param topics:  Dict of (topic_name -> numPartitions) to consume.\n",
    "                        Each partition is consumed in its own thread.\n",
    "        :param kafkaParams: Additional params for Kafka\n",
    "        :param storageLevel:  RDD storage level.\n",
    "        :param keyDecoder:  A function used to decode key (default is utf8_decoder)\n",
    "        :param valueDecoder:  A function used to decode value (default is utf8_decoder)\n",
    "        :return: A DStream object\n",
    "        \"\"\"\n",
    "        if kafkaParams is None:\n",
    "            kafkaParams = {}\n",
    "        kafkaParams.update({\n",
    "            \"kafka.bootstrap.servers\": zkQuorum,\n",
    "            \"group.id\": groupId,\n",
    "            \"zookeeper.connection.timeout.ms\": \"10000\",\n",
    "        })\n",
    "        if storageLevel is None:\n",
    "            storageLevel = ssc._sc._getJavaStorageLevel(ssc._sc.defaultStorageLevel())\n",
    "        stream = ssc._jvm.org.apache.spark.streaming.kafka.KafkaUtils.createStream(\n",
    "            ssc._jssc, kafkaParams, topics, storageLevel)\n",
    "        ser = NoOpSerializer()\n",
    "        stream = DStream(stream, ssc, ser)\n",
    "        return stream.map(lambda k_v: (keyDecoder(k_v[0]), valueDecoder(k_v[1])))\n",
    "\n",
    "    @staticmethod\n",
    "    def createDirectStream(ssc, topics, kafkaParams, fromOffsets=None,\n",
    "                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder,\n",
    "                           messageHandler=None):\n",
    "        \"\"\"\n",
    "        Create an input stream that directly pulls messages from a Kafka Broker and specific offset.\n",
    "\n",
    "        This is not a receiver based Kafka input stream, it directly pulls the message from Kafka\n",
    "        in each batch duration and processed without storing.\n",
    "\n",
    "        This does not use Zookeeper to store offsets. The consumed offsets are tracked\n",
    "        by the stream itself. For interoperability with Kafka monitoring tools that depend on\n",
    "        Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.\n",
    "        You can access the offsets used in each batch from the generated RDDs (see\n",
    "\n",
    "        To recover from driver failures, you have to enable checkpointing in the StreamingContext.\n",
    "        The information on consumed offset can be recovered from the checkpoint.\n",
    "        See the programming guide for details (constraints, etc.).\n",
    "\n",
    "        :param ssc:  StreamingContext object.\n",
    "        :param topics:  list of topic_name to consume.\n",
    "        :param kafkaParams: Additional params for Kafka.\n",
    "        :param fromOffsets: Per-topic/partition Kafka offsets defining the (inclusive) starting\n",
    "                            point of the stream (a dictionary mapping `TopicAndPartition` to\n",
    "                            integers).\n",
    "        :param keyDecoder:  A function used to decode key (default is utf8_decoder).\n",
    "        :param valueDecoder:  A function used to decode value (default is utf8_decoder).\n",
    "        :param messageHandler: A function used to convert KafkaMessageAndMetadata. You can assess\n",
    "                               meta using messageHandler (default is None).\n",
    "        :return: A DStream object\n",
    "        \"\"\"\n",
    "        if fromOffsets is None:\n",
    "            fromOffsets = {}\n",
    "        if messageHandler is None:\n",
    "            ser = NoOpSerializer()\n",
    "            func = lambda k_v: (keyDecoder(k_v[0]), valueDecoder(k_v[1]))\n",
    "            stream = ssc._jvm.org.apache.spark.streaming.kafka.KafkaUtils.createDirectStream(\n",
    "                ssc._jssc, kafkaParams, set(topics), fromOffsets)\n",
    "        else:\n",
    "            ser = PickleSerializer()\n",
    "            func = messageHandler\n",
    "            stream = ssc._jvm.org.apache.spark.streaming.kafka.KafkaUtils.createDirectStream(\n",
    "                ssc._jssc, kafkaParams, set(topics), fromOffsets, func)\n",
    "        stream = DStream(stream, ssc, ser)\n",
    "        return stream\n",
    "\n",
    "    @staticmethod\n",
    "    def createRDD(sc, kafkaParams, offsetRanges, leaders=None,\n",
    "                  keyDecoder=utf8_decoder, valueDecoder=utf8_decoder,\n",
    "                  messageHandler=None):\n",
    "        \"\"\"\n",
    "        Create an RDD from Kafka using offset ranges for each topic and partition.\n",
    "\n",
    "        :param sc:  SparkContext object\n",
    "        :param kafkaParams: Additional params for Kafka\n",
    "        :param offsetRanges:  list of offsetRange to specify topic:partition:[start, end) to consume\n",
    "        :param leaders: Kafka brokers for each TopicAndPartition in offsetRanges.  May be an empty\n",
    "            map, in which case leaders will be looked up on the driver.\n",
    "        :param keyDecoder:  A function used to decode key (default is utf8_decoder)\n",
    "        :param valueDecoder:  A function used to decode value (default is utf8_decoder)\n",
    "        :param messageHandler: A function used to convert KafkaMessageAndMetadata. You can assess\n",
    "                               meta using messageHandler (default is None).\n",
    "        :return: An RDD object\n",
    "        \"\"\"\n",
    "        if leaders is None:\n",
    "            leaders = {}\n",
    "        if messageHandler is None:\n",
    "            ser = NoOpSerializer()\n",
    "            func = lambda k_v: (keyDecoder(k_v[0]), valueDecoder(k_v[1]))\n",
    "            rdd = sc._jvm.org.apache.spark.streaming.kafka.KafkaUtils.createRDD(\n",
    "                sc._jsc, kafkaParams, offsetRanges, leaders)\n",
    "        else:\n",
    "            ser = PickleSerializer()\n",
    "            func = messageHandler\n",
    "            rdd = sc._jvm.org.apache.spark.streaming.kafka.KafkaUtils.createRDD(\n",
    "                sc._jsc, kafkaParams, offsetRanges, leaders, func)\n",
    "        rdd = RDD(rdd, sc, ser)\n",
    "        return rdd.map(func)\n",
    "\n",
    "\n",
    "class OffsetRange(object):\n",
    "    \"\"\"\n",
    "    Represents a range of offsets from a single Kafka TopicAndPartition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic, partition, fromOffset, untilOffset):\n",
    "        \"\"\"\n",
    "        Create an OffsetRange to represent range of offsets\n",
    "        :param topic: Kafka topic name.\n",
    "        :param partition: Kafka partition id.\n",
    "        :param fromOffset: Inclusive starting offset.\n",
    "        :param untilOffset: Exclusive ending offset.\n",
    "        \"\"\"\n",
    "        self.topic = topic\n",
    "        self.partition = partition\n",
    "        self.fromOffset = fromOffset\n",
    "        self.untilOffset = untilOffset\n",
    "\n",
    "\n",
    "class TopicAndPartition(object):\n",
    "    \"\"\"\n",
    "    Represents a specific topic and partition for Kafka.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic, partition):\n",
    "        \"\"\"\n",
    "        Create a Python TopicAndPartition to map to the Java related object\n",
    "        :param topic: Kafka topic name.\n",
    "        :param partition: Kafka partition id.\n",
    "        \"\"\"\n",
    "        self._topic = topic\n",
    "        self._partition = partition\n",
    "\n",
    "\n",
    "class Broker(object):\n",
    "    \"\"\"\n",
    "    Represent the host and port info for a Kafka broker.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, host, port):\n",
    "        \"\"\"\n",
    "        Create a Python Broker to map to the Java related object.\n",
    "        :param host: Broker's hostname.\n",
    "        :param port: Broker's port.\n",
    "        \"\"\"\n",
    "        self._host = host\n",
    "        self._port = port\n",
    "\n",
    "\n",
    "class KafkaMessageAndMetadata(object):\n",
    "    \"\"\"\n",
    "    Kafka message and metadata information. Including topic, partition, offset and message\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic, partition, offset, key, message):\n",
    "        \"\"\"\n",
    "        Python wrapper of Kafka MessageAndMetadata\n",
    "        :param topic: topic name of this Kafka message\n",
    "        :param partition: partition id of this Kafka message\n",
    "        :param offset: Offset of this Kafka message in the specific partition\n",
    "        :param key: key payload of this Kafka message, can be null if this Kafka message has no key\n",
    "                    specified, the return data is undecoded bytearry.\n",
    "        :param message: actual message payload of this Kafka message, the return data is\n",
    "                        undecoded bytearray.\n",
    "        \"\"\"\n",
    "        self.topic = topic\n",
    "        self.partition = partition\n",
    "        self.offset = offset\n",
    "        self._rawKey = key\n",
    "        self._rawMessage = message\n",
    "        self._keyDecoder = utf8_decoder\n",
    "        self._valueDecoder = utf8_decoder\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"KafkaMessageAndMetadata(topic: %s, partition: %d, offset: %d, key and message...)\" \\\n",
    "               % (self.topic, self.partition, self.offset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (KafkaMessageAndMetadata,\n",
    "                (self.topic, self.partition, self.offset, self._rawKey, self._rawMessage))\n",
    "\n",
    "    def _set_key_decoder(self, decoder):\n",
    "        self._keyDecoder = decoder\n",
    "\n",
    "    def _set_value_decoder(self, decoder):\n",
    "        self._valueDecoder = decoder\n",
    "\n",
    "    @property\n",
    "    def key(self):\n",
    "        return self._keyDecoder(self._rawKey)\n",
    "\n",
    "    @property\n",
    "    def message(self):\n",
    "        return self._valueDecoder(self._rawMessage)\n",
    "\n",
    "\n",
    "class KafkaRDD(RDD):\n",
    "    \"\"\"\n",
    "    A Python wrapper of KafkaRDD, to provide additional information on normal RDD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jrdd, ctx, jrdd_deserializer):\n",
    "        RDD.__init__(self, jrdd, ctx, jrdd_deserializer)\n",
    "\n",
    "    def offsetRanges(self):\n",
    "        \"\"\"\n",
    "        Get the OffsetRange of specific KafkaRDD.\n",
    "        :return: A list of OffsetRange\n",
    "        \"\"\"\n",
    "        helper = KafkaUtils._get_helper(self.ctx)\n",
    "        joffsetRanges = helper.offsetRangesOfKafkaRDD(self._jrdd.rdd())\n",
    "        ranges = [OffsetRange(o.topic(), o.partition(), o.fromOffset(), o.untilOffset())\n",
    "                  for o in joffsetRanges]\n",
    "        return ranges\n",
    "\n",
    "\n",
    "class KafkaDStream(DStream):\n",
    "    \"\"\"\n",
    "    A Python wrapper of KafkaDStream\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jdstream, ssc, jrdd_deserializer):\n",
    "        DStream.__init__(self, jdstream, ssc, jrdd_deserializer)\n",
    "\n",
    "    def foreachRDD(self, func):\n",
    "        \"\"\"\n",
    "        Apply a function to each RDD in this DStream.\n",
    "        \"\"\"\n",
    "        if func.__code__.co_argcount == 1:\n",
    "            old_func = func\n",
    "            func = lambda r, rdd: old_func(rdd)\n",
    "        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer) \\\n",
    "            .rdd_wrapper(lambda jrdd, ctx, ser: KafkaRDD(jrdd, ctx, ser))\n",
    "        api = self._ssc._jvm.PythonDStream\n",
    "        api.callForeachRDD(self._jdstream, jfunc)\n",
    "\n",
    "    def transform(self, func):\n",
    "        \"\"\"\n",
    "        Return a new DStream in which each RDD is generated by applying a function\n",
    "        on each RDD of this DStream.\n",
    "\n",
    "        `func` can have one argument of `rdd`, or have two arguments of\n",
    "        (`time`, `rdd`)\n",
    "        \"\"\"\n",
    "        if func.__code__.co_argcount == 1:\n",
    "            oldfunc = func\n",
    "            func = lambda t, rdd: oldfunc(rdd)\n",
    "        assert func.__code__.co_argcount == 2, \"func should take one or two arguments\"\n",
    "\n",
    "        return KafkaTransformedDStream(self, func)\n",
    "\n",
    "\n",
    "class KafkaTransformedDStream(TransformedDStream):\n",
    "    \"\"\"\n",
    "    Kafka specific wrapper of TransformedDStream to transform on Kafka RDD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prev, func):\n",
    "        TransformedDStream.__init__(self, prev, func)\n",
    "\n",
    "    @property\n",
    "    def _jdstream(self):\n",
    "        if self._jdstream_val is not None:\n",
    "            return self._jdstream_val\n",
    "\n",
    "        jfunc = TransformFunction(self._sc, self.func, self.prev._jrdd_deserializer) \\\n",
    "            .rdd_wrapper(lambda jrdd, ctx, ser: KafkaRDD(jrdd, ctx, ser))\n",
    "        dstream = self._sc._jvm.PythonTransformedDStream(self.prev._jdstream.dstream(), jfunc)\n",
    "        self._jdstream_val = dstream.asJavaDStream()\n",
    "        return self._jdstream_val\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
